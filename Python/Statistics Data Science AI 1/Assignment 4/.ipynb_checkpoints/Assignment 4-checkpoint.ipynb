{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b9aeb5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Assignment 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c959cf9",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "####  In this problem, you will predict tumor type from gene expression data. Since there are many more gene features than observations of patients, we will use ridge and LASSO regularization for logistic regression to reduce overfitting and help select the most relevant features out of a large group of features. This dataset has a multi-class outcome variable. The possible tumor types are BRCA, COAD, KIRC, LUAD, or PRAD. You will analyze this dataset by building a multinomial regression model with 1 and 2 regularization. The recommended approach is the glmnet package in R, which is covered in the code in class. You can check the “Multinomial Regression” section found at this link for specific information about multinomial regression in glmnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882018a",
   "metadata": {},
   "source": [
    "#### (a) Load the labels and data with read.csv. Remove any columns with missing entries. Remove any columns with variance less than 0.001. Standardize each gene predictor column to have mean 0 and standard deviation 1 (this is important when doing regularized regression). Split the dataset randomly into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50aae329",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gene_0    gene_1    gene_2    gene_3     gene_4  gene_5    gene_6  \\\n",
      "0     0.0  2.017209  3.265527  5.478487  10.431999     0.0  7.175175   \n",
      "1     0.0  0.592732  1.588421  7.586157   9.623011     0.0  6.816049   \n",
      "2     0.0  3.511759  4.327199  6.881787   9.870730     0.0  6.972130   \n",
      "3     0.0  3.663618  4.507649  6.659068  10.196184     0.0  7.843375   \n",
      "4     0.0  2.655741  2.821547  6.539454   9.738265     0.0  6.566967   \n",
      "\n",
      "     gene_7  gene_8  gene_9  ...  gene_20521  gene_20522  gene_20523  \\\n",
      "0  0.591871     0.0     0.0  ...    4.926711    8.210257    9.723516   \n",
      "1  0.000000     0.0     0.0  ...    4.593372    7.323865    9.740931   \n",
      "2  0.452595     0.0     0.0  ...    5.125213    8.127123   10.908640   \n",
      "3  0.434882     0.0     0.0  ...    6.076566    8.792959   10.141520   \n",
      "4  0.360982     0.0     0.0  ...    5.996032    8.891425   10.373790   \n",
      "\n",
      "   gene_20524  gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  \\\n",
      "0    7.220030    9.119813   12.003135    9.650743    8.921326    5.286759   \n",
      "1    6.256586    8.381612   12.674552   10.517059    9.397854    2.094168   \n",
      "2    5.401607    9.911597    9.045255    9.788359   10.090470    1.683023   \n",
      "3    8.942805    9.601208   11.392682    9.694814    9.684365    3.292001   \n",
      "4    7.181162    9.846910   11.922439    9.217749    9.461191    5.110372   \n",
      "\n",
      "   gene_20530  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n",
      "\n",
      "[5 rows x 20531 columns]\n",
      "   Class\n",
      "0    4.0\n",
      "1    3.0\n",
      "2    4.0\n",
      "3    4.0\n",
      "4    0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#print(\"Columns with Variance < 0.001\")\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#for features in concol:\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#    print(features)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m x_reduced \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdrop(concol,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m concol, selector, features\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#%% Step 3 - Standardize the Data\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "#%% Step 1 - Load Data & Prep Data\n",
    "import pandas as pd;\n",
    "\n",
    "x = pd.read_table('C:/Users/danma/Downloads/gene_data.csv', sep=\",\",)\n",
    "y = pd.read_table('C:/Users/danma/Downloads/gene_labels.csv', sep=\",\",)\n",
    "\n",
    "#possibly remove sample name column\n",
    "x = x.iloc[: , 1:]\n",
    "y = y.iloc[: , 1:]\n",
    " \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "y = enc.fit_transform(y)\n",
    "y = pd.DataFrame(y, columns = [\"Class\"])\n",
    "\n",
    "print(x.head())\n",
    "print(y.head())\n",
    "#%% Step 2 - Remove Variance < 0.001\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold = 0.001)\n",
    "selector.fit(x)\n",
    "\n",
    "concol = [column for column in x.columns \n",
    "          if column not in x.columns[selector.get_support()]]\n",
    "\n",
    "#print(\"Columns with Variance < 0.001\")\n",
    "#for features in concol:\n",
    "#    print(features)\n",
    "    \n",
    "x_reduced = x.drop(concol,axis=1)\n",
    "\n",
    "del concol, selector\n",
    "#%% Step 3 - Standardize the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_reduced)\n",
    "\n",
    "x_scaled = pd.DataFrame(x_scaled, columns = x_reduced.columns)#return to dataframe with column names\n",
    "\n",
    "del x_reduced, scaler, x\n",
    "#%% Step 4 - Split Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y)\n",
    "print(x_train.dtypes)\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de1c64",
   "metadata": {},
   "source": [
    "#### (b) Use ridge logistic regression with 10-fold cross validation to model the response given the gene expression predictors. What is your optimal value of the regularization parameter λ? Apply your model to give predictions using the optimal value of λ. Make a confusion matrix showing the accuracy of your model on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 4.5 Testing Logistic Regression and RidgeClassification to ensure that they yield the same results, which they did meaning that Ridge as a package alone works for L2 Regularization while running quicker than both.\n",
    "#from sklearn.linear_model import LogisticRegressionCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#clf = LogisticRegressionCV(cv=10, random_state=0, Cs=list_alphas, penalty=\"l2\", solver=\"saga\", n_jobs=-1, multi_class=\"multinomial\", max_iter=5000).fit(x_train, y_train.values.ravel())\n",
    "#print(clf.Cs)\n",
    "\n",
    "#y_ridge_pred = clf.predict(x_test)  \n",
    "#from sklearn.linear_model import RidgeClassifierCV\n",
    "#list_alphas = [1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3,1e-2, 1e-1, 1, 5, 10, 20]\n",
    "#clf = RidgeClassifierCV(cv=10, alphas=list_alphas).fit(x_train, y_train.values.ravel())\n",
    "#print(\"Ridge optimal λ = \",clf.alpha_)\n",
    "\n",
    "#y_ridge_pred = clf.predict(x_test)\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#using ravel to create single array and round to create classifier prediction for confusion matrix\n",
    "#print(confusion_matrix(y_test.values.ravel(),y_ridge_pred.round(0)))\n",
    "#matrix shows exact match\n",
    "\n",
    "#%% Step 5 - Ridge Regression, 10 Fold, Optimal Lambda, Confusion Matrix Accuracy\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "#list_alphas = np.logspace(-15, 1.35, 400) ran large range but defaulted at the 1e-15 after a 5 min run\n",
    "list_alphas = [1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3,1e-2, 1e-1, 1, 5, 10, 20]\n",
    "clf = RidgeCV(cv=10, alphas=list_alphas).fit(x_train, y_train.values.ravel())\n",
    "print(\"Ridge optimal λ = \",clf.alpha_)\n",
    "\n",
    "y_ridge_pred = clf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#using ravel to create single array and round to create classifier prediction for confusion matrix\n",
    "cm = confusion_matrix(y_test.values.ravel(),y_ridge_pred.round(0))\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test.values.ravel(),y_ridge_pred.round(0))\n",
    "disp.ax_.set_title(\"Ridge Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46949aa",
   "metadata": {},
   "source": [
    "#### (c) Use LASSO logistic regression with 10-fold cross validation to model the response given the gene expression predictors. What is your optimal value of the regularization parameter λ? Apply your model to give predictions using the optimal value of λ. Make a confusion matrix showing the accuracy of your model on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62193893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 6 - LASSO Regression, 10 Fold, Optimal Lambda, Confusion Matrix Accuracy\n",
    "from sklearn.linear_model import LassoCV\n",
    "list_alphas = np.logspace(-15, 1.35, 400)\n",
    "reg = LassoCV(cv=10, alphas=list_alphas, n_jobs=-1, positive=True).fit(x_train, y_train.values.ravel())\n",
    "print(\"LASSO optimal λ = \",reg.alpha_)\n",
    "\n",
    "y_lasso_pred = reg.predict(x_test)\n",
    "\n",
    "cm = confusion_matrix(y_test.values.ravel(),y_lasso_pred.round(0))\n",
    "print(cm)\n",
    "#matrix shows incorrect guesses and a wider range\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test.values.ravel(),y_lasso_pred.round(0))\n",
    "disp.ax_.set_title(\"LASSO Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34002a",
   "metadata": {},
   "source": [
    "#### (d) Give a list of the top 20 most relevant genes that are selected by your LASSO model at the optimal value of λ. The coefficients for a multinomial regression model will be a p × C matrix where C is the number of classes and p is the number of feature columns. What relation do your selected genes have to tumor expression? You can determine this by looking at which of the C coefficients associated with a certain gene are non-zero. Positive values in a certain index correspond to a high probability of the tumor associated with that index, while negative values correspond to a lower probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 7 - 20 Most Relevant Predictors from LASSO\n",
    "#%% Step 7.1 - Fitting Logistic Regression to get Coef for all Tumors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#fitting Logistic Regression for All Coefficients\n",
    "lasso = LogisticRegression(C=(1/reg.alpha_), penalty=\"l1\", solver=\"saga\", n_jobs=-1, max_iter=5000)\n",
    "lasso.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = lasso.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test.values.ravel(),y_pred)\n",
    "disp.ax_.set_title(\"Logistic(L1) Confusion Matrix\")\n",
    "\n",
    "#%% Step 7.2 - Pull Coefficient Matrix from Logistic Regression\n",
    "imp = lasso.coef_\n",
    "imp = imp.transpose()\n",
    "#%% Step 7.3 - Coefficients from LASSO model\n",
    "import numpy as np\n",
    "importance = pd.DataFrame({'gene': reg.feature_names_in_, 'coef': reg.coef_}, columns=['gene', 'coef'])\n",
    "importancesorted = importance.sort_values(by=['coef'], ascending=False)\n",
    "top20 = importancesorted.head(20)\n",
    "print(\"Top 20 Most Relevant Genes from LASSO:\\n\")\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top20, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "#%% Step 8 - Relation of Genes to Tumors\n",
    "#%% Step 8.1 - Top20(LASSO) from LogReg\n",
    "cat = enc.categories_[0]\n",
    "importance[enc.categories_[0]] = imp\n",
    "importancesorted = importance.sort_values(by=['coef'], ascending=False)\n",
    "top20 = importancesorted.head(20)\n",
    "print(\"Top 20 Most Relevant Genes from LASSO with Coefficients from LogReg(L1):\\n\")\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top20, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "#%% Step 8.2 - Relation\n",
    "#swap negatives for low, positives for high, zeros for none\n",
    "values = top20.iloc[: , -5:]\n",
    "values[values < 0] = -1\n",
    "values[values > 0] = 1\n",
    "\n",
    "mapping = {1:'high prob', -1:'low prob', 0:'no relation'}\n",
    "values = values.astype(int).replace({'BRCA': mapping, 'COAD': mapping, 'KIRC': mapping, 'LUAD': mapping, 'PRAD': mapping})\n",
    "#drop coef column, and print new dataframe with labels rather than numerical coef\n",
    "values.insert (0, 'gene', top20['gene'])\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(values, headers='keys', tablefmt='fancy_grid', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679b114",
   "metadata": {},
   "source": [
    "#### Note: If columns are highly correlated, LASSO will often arbitrarily select a single column, so a full report of relevent genes would involve predictors selected by LASSO and genes that are highly correlated. Other techniques like group LASSO can select subsets of related genes, these will not be covered in this class. You could also try to combine the 1 and 2 penalties to get representation of meaningful predictors that are also correlated (see the reference material above)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
